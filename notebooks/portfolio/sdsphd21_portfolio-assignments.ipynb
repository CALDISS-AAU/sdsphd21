{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "883tPg8mDseq"
   },
   "source": [
    "# Portfolio for SDSPhD21 (draft)\n",
    "\n",
    "This notebook contains the exercises and assignments to be answered in a portfolio for the PhD course \"Social Data Science: An Applied Introduction to Machine Learning\" at Aalborg University, November 2021.\n",
    "\n",
    "Each day of the course you are given an hour to work on a portfolio with the possibility of sparring with the course lecturers. \n",
    "\n",
    "You are expected to attempt to solve the various assignments using the methods and tools taught during the course. Answers should be combined into a notebook (fx by adding answers to a copy of this one). \n",
    "\n",
    "\n",
    "#### How to hand in your portfolio notebooks\n",
    "\n",
    "You can hand in your portfolio notebooks in two ways:\n",
    "\n",
    "- Saving your notebooks in a GitHub repository and then sending the repository URL to the course organizer (Kristian Kjelmann)\n",
    "- Sharing your notebooks directly with the course organizer (Kristian Kjelmann) in Google Colab.\n",
    "\n",
    "Kristian’s e-mail: kgk@adm.aau.dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDgEJEMSDser"
   },
   "source": [
    "# Portfolio assignments for Monday (\"Introduction to Machine Learning\")\n",
    "\n",
    "**Requirement**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obd04ZJGDseu"
   },
   "source": [
    "---\n",
    "\n",
    "## Clustering\n",
    "\n",
    "I have created a larger set of variables from the Danish Value Study from 1999. You can find data here:\n",
    "\n",
    "https://raw.githubusercontent.com/CALDISS-AAU/sdsphd20/master/datasets/value99.csv\n",
    "\n",
    "In all examples, values towards 1 is agree a lot and values towards 5 is disagree a lot.\n",
    "\n",
    "As an example: \"Does not want alchoholics as neighbors\" --> 1=does not want, 2=doesnt care\n",
    "\n",
    "Or: Trust to the military --> 1=Trust very much, 2= Trust some, 3=Trust a little 4=Does not trust at all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myBvN4jaDseu"
   },
   "source": [
    "[![2hAEhX.md.png](https://iili.io/2hAEhX.md.png)](https://freeimage.host/i/2hAEhX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTMbEvvDDseu"
   },
   "source": [
    "Pick some varibles you think is interesting and play with creating clusters. Can we explain what is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiQDKkkvDseu"
   },
   "outputs": [],
   "source": [
    "# Your solutions from here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio assignment for Tuesday (\"Introduction to Supervised Machine Learning\" and \"Working with geospatial data\")\n",
    "\n",
    "**Requirement**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDcrUan2Dseu"
   },
   "source": [
    "## Supervised Machine Learning: Employee turnover\n",
    "\n",
    "### The assignment\n",
    "\n",
    "In the repo, you will find a dataset describing employee turnover in a company.\n",
    "\n",
    "https://raw.githubusercontent.com/CALDISS-AAU/sdsphd20/master/datasets/turnover.csv\n",
    "\n",
    "The dataset contains data collected in an employee survey and enriched with HR data.\n",
    "\n",
    "The variable `churn` tells us if the employee left the company in the past 3 months. The other variables are collected\n",
    "\n",
    "#### Classification\n",
    "\n",
    "Try to predict `churn` using a classification pipeline (perhaps add some simple exploration of the data first)\n",
    "\n",
    "#### Regression\n",
    "Try to predict the number of weekly average hours worked.\n",
    "\n",
    "**Before** working with the data, you should use `pd.get_dummies` to get dummies for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Nw_nF0XDseu"
   },
   "outputs": [],
   "source": [
    "# Your solutions from here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHOor6pVDsey"
   },
   "source": [
    "---\n",
    "\n",
    "## Working with geospatial data: Noise and mental health\n",
    "\n",
    "So, now I have a better hypothesis; mental health has something to do with noise!\n",
    "\n",
    "Here, you find a new geo-dataset;\n",
    "\n",
    "- OPEN_DATA_STOEJDATA_VIEWPoint.shp\n",
    "\n",
    "This contains information about noise at different places within municipalities. The end goal is to create a map where each point is aggregated to municipality level and we visualize where the noise is more severe. We use the column \"GNSHAST071\" to measure average noice at that point.\n",
    "\n",
    "When that map is done, create a pretty map with the mental health measurement and compare the two - are there any connection between noise at an aggregated level and mental health?\n",
    "\n",
    "Feel free to play around with types of basemaps, types of colors and all sorts of things - the goal is not to become the leading expert in making fancy maps but to have fun and learn stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkc6QeGlDsey"
   },
   "outputs": [],
   "source": [
    "!pip install geopandas # geopandas is not installed by default on Colab - this installs it\n",
    "!pip install contextily # contextily is not installed by default on Colab - this installs it\n",
    "!pip install pygeos # pygeos optional dependency for geopandas (which we will use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmcwKlvsDsey"
   },
   "outputs": [],
   "source": [
    "# Download and unzip shape files\n",
    "\n",
    "!wget \"https://github.com/CALDISS-AAU/sdsphd20/raw/master/notebooks/wed25/Shapefiles/shapefiles_exercise.zip\"\n",
    "!unzip shapefiles_exercise.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OreYm-p1Dsey"
   },
   "outputs": [],
   "source": [
    "# Loading required packages\n",
    "\n",
    "import geopandas as gdp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Proj\n",
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_klJsz29Dsey"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "stoejdata = gdp.read_file(\"OPEN_DATA_STOEJDATA_VIEWPoint.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fp2YEoqkDsey"
   },
   "outputs": [],
   "source": [
    "# Your solutions from here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zcz__jbzDseu"
   },
   "source": [
    "---\n",
    "\n",
    "# Portfolio assignments for Wednesday (\"Introduction to Network Analysis\")\n",
    "\n",
    "**Requirement:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqUNq8XA4Xfb"
   },
   "source": [
    "## Network analysis: Case Study 1: Directed Networks: Friends & Foes at Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUGY4_tm6gKv"
   },
   "source": [
    "### Introduction to the case\n",
    "\n",
    "* Emmanuel Lazega, The Collegial Phenomenon: The Social Mechanisms of Cooperation Among Peers in a Corporate Law Partnership, Oxford University Press (2001).\n",
    "\n",
    "#### Data \n",
    "This data set comes from a network study of corporate law partnership that was carried out in a Northeastern US corporate law firm, referred to as SG&R, 1988-1991 in New England. It includes (among others) measurements of networks among the 71 attorneys (partners and associates) of this firm, i.e. their strong-coworker network, advice network, friendship network, and indirect control networks. Various members' attributes are also part of the dataset, including seniority, formal status, office in which they work, gender, lawschool attended, individual performance measurements (hours worked, fees brought in), attitudes concerning various management policy options, etc. This dataset was used to identify social processes such as bounded solidarity, lateral control, quality control, knowledge sharing, balancing powers, regulation, etc. among peers.\n",
    "\n",
    "#### Setting\n",
    "* What do corporate lawyers do? Litigation and corporate work.\n",
    "* Division of work and interdependencies.\n",
    "* Three offices, no departments, built-in pressures to grow, intake and assignment rules.\n",
    "* Partners and associates: hierarchy, up or out rule, billing targets.\n",
    "* Partnership agreement (sharing benefits equally, 90% exclusion rule, governance structure, elusive committee system) and incompleteness of the contracts.\n",
    "* Informal, unwritten rules (ex: no moonlighting, no investment in buildings, no nepotism, no borrowing to pay partners, etc.).\n",
    "* Huge incentives to behave opportunistically ; thus the dataset is appropriate for the study of social processes that make cooperation among rival partners possible. \n",
    "* Sociometric name generators used to elicit coworkers, advice, and 'friendship' ties at SG&R:\"Here is the list of all the members of your Firm.\"\n",
    "\n",
    "The networks where created according to the follwoing questionaire:\n",
    "\n",
    "* Strong coworkers network: \"Because most firms like yours are also organized very informally, it is difficult to get a clear idea of how the members really work together. Think back over the past year, consider all the lawyers in your Firm. Would you go through this list and check the names of those with whom you have worked with. By \"worked with\" I mean that you have spent time together on at least one case, that you have been assigned to the same case, that they read or used your work product or that you have read or used their work product; this includes professional work done within the Firm like Bar association work, administration, etc.\"\n",
    "* Basic advice network: \"Think back over the past year, consider all the lawyers in your Firm. To whom did you go for basic professional advice? For instance, you want to make sure that you are handling a case right, making a proper decision, and you want to consult someone whose professional opinions are in general of great value to you. By advice I do not mean simply technical advice.\"\n",
    "* 'Friendship' network:\n",
    "\"Would you go through this list, and check the names of those you socialize with outside work. You know their family, they know yours, for instance. I do not mean all the people you are simply on a friendly level with, or people you happen to meet at Firm functions.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqZlW3YcDseu"
   },
   "source": [
    "### Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ac9ru1aDseu"
   },
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVpLaHWODseu"
   },
   "outputs": [],
   "source": [
    "# Installing visualization packages\n",
    "!pip install -U bokeh\n",
    "!pip install -q holoviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zK3ODrdDseu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import itertools # Python's amazing iteration & combination library\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XEoyW1qDsev"
   },
   "outputs": [],
   "source": [
    "# Visualization defaults\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "from bokeh.plotting import show\n",
    "\n",
    "# Setting the default figure size a bit larger\n",
    "defaults = dict(width=750, height=750, padding=0.1,\n",
    "                xaxis=None, yaxis=None)\n",
    "hv.opts.defaults(\n",
    "    opts.EdgePaths(**defaults), opts.Graph(**defaults), opts.Nodes(**defaults))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if1cPg6k6imS"
   },
   "source": [
    "####  Load the data\n",
    "\n",
    "Lets load the data! The three networks refer to cowork, friendship, and advice. The first 36 respondents are the partners in the firm.\n",
    "\n",
    "(the cell belows reads in the tables, performs some recoding and cleanup and creates network objects for the 3 data sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71deTXch7iSr"
   },
   "outputs": [],
   "source": [
    "mat_friendship = pd.read_table(\"https://www.dropbox.com/s/0saiulir3pr566k/ELfriend.dat?dl=1\", delim_whitespace=True, header=None) \n",
    "mat_advice = pd.read_table(\"https://www.dropbox.com/s/apq42n1grim23k9/ELadv.dat?dl=1\", delim_whitespace=True, header=None) \n",
    "mat_work = pd.read_table(\"https://www.dropbox.com/s/dliz0sd7or8tv01/ELwork.dat?dl=1\", delim_whitespace=True, header=None)\n",
    "\n",
    "G_friendship = nx.from_pandas_adjacency(mat_friendship, create_using=nx.DiGraph)\n",
    "G_advice = nx.from_pandas_adjacency(mat_advice, create_using=nx.DiGraph)\n",
    "G_work = nx.from_pandas_adjacency(mat_work, create_using=nx.DiGraph)\n",
    "\n",
    "attributes = pd.read_table(\"https://www.dropbox.com/s/qz7fvfgx8lvjgpr/ELattr.dat?dl=1\", delim_whitespace=True, header=None, dtype='int') \n",
    "attributes=attributes.round().astype(int)\n",
    "attributes.columns = [\"id\", \"seniority\", \"gender\", \"office\", \"tenure\", \"age\", \"practice\", \"school\"]\n",
    "attributes.set_index('id',inplace=True)\n",
    "\n",
    "cleanup_nums = {\"seniority\":     {1: \"Partner\", 2: \"Associate\"},\n",
    "                \"gender\":     {1: \"Male\", 2: \"Female\"},\n",
    "                \"office\":     {1: \"Boston\", 2: \"Hartford\", 3:\"Providence\"},\n",
    "                \"practice\":     {1: \"Litigation\", 2: \"Corporate\"},\n",
    "                \"school\":     {1: \"Harvard, Yale\", 2: \"Ucon\", 3: \"Others\"}\n",
    "                } \n",
    "attributes.replace(cleanup_nums, inplace=True)\n",
    "\n",
    "attributes_dict=attributes.T.to_dict()\n",
    "\n",
    "nx.set_node_attributes(G_friendship, attributes_dict)\n",
    "nx.set_node_attributes(G_advice, attributes_dict)\n",
    "nx.set_node_attributes(G_work, attributes_dict)\n",
    "\n",
    "print(nx.get_node_attributes(G_friendship, 'seniority'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWsWIvDeMS3o"
   },
   "source": [
    "#### Calculate dimensional centralities\n",
    "\n",
    "There might be better ways to do that (still experimenting), but for now lets first create centralities upfront for all networks. We for now only look at the in-degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgkLkBk1YbXS"
   },
   "outputs": [],
   "source": [
    "cent_degree_friendship = dict(G_friendship.in_degree)\n",
    "cent_degree_advice = dict(G_advice.in_degree)\n",
    "cent_degree_work = dict(G_work.in_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hJAhy_yY0Xq"
   },
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G_friendship, cent_degree_friendship, 'cent_degree')\n",
    "nx.set_node_attributes(G_advice, cent_degree_advice, 'cent_degree')\n",
    "nx.set_node_attributes(G_work, cent_degree_work, 'cent_degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88gPV3VKMxOT"
   },
   "outputs": [],
   "source": [
    "# Create and save a layout.\n",
    "G_layout = nx.layout.kamada_kawai_layout(G_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6a5uk1rM8Yl"
   },
   "outputs": [],
   "source": [
    "g_plot = hv.Graph.from_networkx(G_friendship, G_layout).opts(tools=['hover'],\n",
    "                                                                        directed=True,\n",
    "                                                                        edge_alpha=0.25,\n",
    "                                                                        node_size='cent_degree',\n",
    "                                                                        #node_color='seniority', cmap='Set1',\n",
    "                                                                        legend_position='right'\n",
    "                                                                        )\n",
    "\n",
    "show(hv.render(g_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXH5FO3BTEKz"
   },
   "outputs": [],
   "source": [
    "g_plot = hv.Graph.from_networkx(G_advice, G_layout).opts(tools=['hover'],\n",
    "                                                                        directed=True,\n",
    "                                                                        edge_alpha=0.25,\n",
    "                                                                        node_size='cent_degree',\n",
    "                                                                        #node_color='cent_degree', cmap='Set1',\n",
    "                                                                        legend_position='right')\n",
    "show(hv.render(g_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBtSaFIEcV6D"
   },
   "outputs": [],
   "source": [
    "g_plot = hv.Graph.from_networkx(G_work, G_layout).opts(tools=['hover'],\n",
    "                                                                        directed=True,\n",
    "                                                                        edge_alpha=0.25,\n",
    "                                                                        node_size='cent_degree',\n",
    "                                                                        #node_color='seniority', cmap='Set1',\n",
    "                                                                        legend_position='right')\n",
    "show(hv.render(g_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvmBMVQm7vnK"
   },
   "source": [
    "#### Assortiativity\n",
    "\n",
    "We can also calculate another interested measure, particularly in social networks: Assortiativity. In a nutshell, it measures if two nodes that share certain characteristics ahve a higher or lower probability to be connected.\n",
    "\n",
    "For details, check:\n",
    "\n",
    "* Newman, M. E. J. (27 February 2003). \"Mixing patterns in networks\". Physical Review E. American Physical Society (APS). 67 (2): 026126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWKHXIc5skJL"
   },
   "outputs": [],
   "source": [
    "nx.attribute_assortativity_coefficient(G_friendship, 'seniority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4gz_Wbg7UZT"
   },
   "outputs": [],
   "source": [
    "nx.attribute_assortativity_coefficient(G_friendship, 'school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FK9n01K47d_D"
   },
   "outputs": [],
   "source": [
    "nx.attribute_assortativity_coefficient(G_friendship, 'office')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbOzP1X_71pH"
   },
   "source": [
    "#### Reciprocity\n",
    "\n",
    "Anotyher interesting question usually is, if directed edges are reciptocated, meaning that an edge between `i,j` makes an edge between `j,i` more likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unMN2vQp7_Ka"
   },
   "outputs": [],
   "source": [
    "nx.overall_reciprocity(G_friendship)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8Ats3vnDsew"
   },
   "source": [
    "### The assignment\n",
    "\n",
    "Explore the network further.\n",
    "\n",
    "1. Calculate the reciprocity for the work and advise network. Are the numbers diffetrent? Why might that be the case?\n",
    "2. Identify communities in the friendship and advice network (hint: works only on undirected networks, so you might have to create an undirected graph)\n",
    "3. Visualize these communities (static or dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnZEIsX5Dsew"
   },
   "outputs": [],
   "source": [
    "# Your solutions here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xwi3wsmKDsey"
   },
   "source": [
    "# Portfolio assignments for Thursday (\"Text as Data\" and \"Machine Learning and Text\")\n",
    "\n",
    "**Requirement:** ...\n",
    "\n",
    "## NLP: Trump vs. GPT-2\n",
    "\n",
    "The site [https://faketrump.ai/](https://faketrump.ai/) WAS an interesting example of AI-powered fake-text generation. They wrote in 2019:\n",
    "\n",
    ">We built an artificial intelligence model by fine-tuning [GPT-2](https://openai.com/blog/better-language-models/) to generate tweets in the style of Donald Trump’s Twitter account. After seeing the results, we also built a discriminator that can accurately detect fake tweets 77% of the time — think you can beat our classifier? Try it yourself!\n",
    "\n",
    "Unfortunately, they decided to take down the site and the dataset.\n",
    "\n",
    "GPT-2 is a neural transformer-based model, that has been announced by OpenAI in February 2019 and created considerable discussion because they decided - in contrast to their earlier policies - not to release the mode to the public. Their central argument was that the model could be used to produce fake news, spam and alike too easily. The footnote of the faketrump page reads: “Generating realistic fake text has become much more accessible. We hope to highlight the current state of text generation to demonstrate how difficult it is to discern fiction from reality.”\n",
    "\n",
    "\n",
    "Since then several organizations and researchers have shown that it is [possible to develop systems to detect “fake text”](https://www.theguardian.com/technology/2019/jul/04/ai-fake-text-gpt-2-concerns-false-information). We believe that you too can implement a competitive system.\n",
    "\n",
    "Having no dataset from that project, Roman decided to retrain GPT2 to generate new fake trump tweets. If they can do that, we can do that! However, it seems as if it is easier for ML models to identify our fake tweets...well...they are an AI company and probably spent more time on that...\n",
    "\n",
    "> I’ve just watched Democrats scream over and over again about trying to Impeach the President of the United States. The Impeachment process is a sham.\n",
    "\n",
    "> The Media must understand!“The New York Times is the leader on a very important subject: How to Combat Trump.” @foxandfriendsSo pathetic! @foxandfriendsI don’t think so.\n",
    "\n",
    "> He is going to do it soon, and with proper borders. Border security is my top priority.The Democrats have failed the people of Arizona in everything else they have done, even their very good immigration laws. They have no sense.\n",
    "\n",
    "The data can be found [here](https://github.com/SDS-AAU/SDS-master/raw/e2c959494d53859c1844604bed09a28a21566d0f/M3/assignments/trump_vs_GPT2.gz) and has the following format:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td>0\n",
    "   </td>\n",
    "   <td>1\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>string\n",
    "   </td>\n",
    "   <td>boolean\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "There are 7368 real Trump tweet and 7368 fake ones.\n",
    "\n",
    "you can open it with:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "data = pd.read_json('https://github.com/SDS-AAU/SDS-master/raw/e2c959494d53859c1844604bed09a28a21566d0f/M3/assignments/trump_vs_GPT2.gz')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Split the data and preprocess it, vectorizing the text using different approaches (BoW, TFIDF, LSI)\n",
    "\n",
    "* Create a system that can identify the fake Trump tweets using LogisticRefression or other classifiers (Sklearn - If you like also more complex models with FastAI, Keras neural nets or alike)\n",
    "\n",
    "* Explore a subset (~1000) of the real and fake tweets using LDA and visualize your exploration\n",
    "\n",
    "* Consider exploring using a different approach (LSI + clustering) or perhaps even [CorEx](https://github.com/gregversteeg/corex_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBcpOT0BGCwv"
   },
   "outputs": [],
   "source": [
    "# Your solutions from here..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "sdsphd20_portfolio-assignments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
