{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to nlp and supervised tasks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMgtXjbUTJgKlAXPnOhivik",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CALDISS-AAU/sdsphd21/blob/master/notebooks/Intro_to_nlp_and_supervised_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIeo10iok3ji"
      },
      "source": [
        "# Intro To NLP vs. Supervised ML\n",
        "\n",
        "Roman Jurowetzki, Aalborg University In part based on the Intro from the DeepNLP course by Dan Anastasyev - https://github.com/DanAnastasyev/DeepNLP-Course\n",
        "\n",
        "In this notebook we are going to explore supervised ML as used on vectorised text input. This is probably the most common application when working with NLP today and very useful if you want to generate (predict) indicators from text data for further exploration (e.g. simple statistical or econometric analysis)\n",
        "\n",
        "We are going to use a (VERY!) standard dataset of movie reviews from IMDB and try solve a binary classification problem - is the movie good or bad. This is certainly a oversimplification but appropriate given the timeframe and that this here is an intro...\n",
        "\n",
        "![alt text](https://media.giphy.com/media/7jNeb9CVSgyUE/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOZ5Zfs-liKY"
      },
      "source": [
        "In this tutorial we will be using the well known IMDB movie review dataset for simple classification with different vectorization techniques:\n",
        "\n",
        "\n",
        "*   Simple bag-of-words\n",
        "*   TF-IDF\n",
        "*   LSI / SVD\n",
        "\n",
        "\n",
        "We will also look at some more recent approaches to model explainability i.e. \"Why did the model decide this or that?\"\n",
        "\n",
        "\n",
        "Finally, we will look at a simple approach to building a **semantic search** based on vector-similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI8jKgTahy_h"
      },
      "source": [
        "!pip -q install eli5 #installing a great package for explaining ML models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fclIC86zkck-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRWx2rDZlQnj"
      },
      "source": [
        "data = pd.read_csv(\"https://github.com/RJuro/nlp-intro-cuny/raw/master/images/imdb.zip\", sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH7R9DAuR2m8"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5kPxU1GlMq3"
      },
      "source": [
        "# some basic text cleaning, removing HTML fragments (only a problem here)\n",
        "\n",
        "import re\n",
        "\n",
        "pattern = re.compile('<br /><br />')\n",
        "\n",
        "print(data['review'].iloc[3])\n",
        "print(pattern.subn(' ', data['review'].iloc[3])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn7LcPeSlioj"
      },
      "source": [
        "# application of the cleaning mask to everthing\n",
        "\n",
        "data['review'] = data['review'].apply(lambda text: pattern.subn(' ', text)[0])\n",
        "data['review'] = data['review'].apply(lambda text: pattern.subn(' ', text)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTqvPlfDl0nE"
      },
      "source": [
        "## Approach 1 - Sklearn\n",
        "If you don't want to deal with language or much code you can just do that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ngrEx9TmnFF"
      },
      "source": [
        "# module to split data into training / test\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvznF1CSmXIL"
      },
      "source": [
        "# define in and outputs\n",
        "\n",
        "X = data['review'].values\n",
        "y = data['is_positive'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDwG-l46mh3z"
      },
      "source": [
        "# Split the data in 80% trainig 20% test\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEGnAYlhloD1"
      },
      "source": [
        "# Simple BoW vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec_1 = vectorizer.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EldgfOcUmqcy"
      },
      "source": [
        "# Instantiate a logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(max_iter=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCn66-mtmu4L"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(X_train_vec_1, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-hOmhrNmxUI"
      },
      "source": [
        "# Transform the test-set\n",
        "X_test_vec_1 = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgDqVUPvm4w0"
      },
      "source": [
        "# Check performance of the model\n",
        "model.score(X_test_vec_1, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3Cq43XoCKda"
      },
      "source": [
        "# Predict on new data\n",
        "\n",
        "y_pred = model.predict(X_test_vec_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAykL7SeES5Y"
      },
      "source": [
        "# confusion matrix by hand... :-)\n",
        "\n",
        "pd.crosstab(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAPhQuK-V_Xb"
      },
      "source": [
        "# Or TFIDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec_2 = vectorizer.fit_transform(X_train)\n",
        "\n",
        "model = LogisticRegression(max_iter=2000)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "model.fit(X_train_vec_2, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1OLUjTyWRP5"
      },
      "source": [
        "# Transform the test-set\n",
        "X_test_vec_2 = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgn5U47KWVMI"
      },
      "source": [
        "# Check performance of the model\n",
        "model.score(X_test_vec_2, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4TCJnC1EXK5"
      },
      "source": [
        "import eli5\n",
        "eli5.show_weights(model, feature_names=vectorizer.get_feature_names(), target_names=['negative','positive'], top=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlAFHvugE6Aa"
      },
      "source": [
        "eli5.show_prediction(model, X_test[0], vec=vectorizer, target_names=['negative','positive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zouCfnRJNoX2"
      },
      "source": [
        "# Let's fire up spacy\n",
        "\n",
        "import spacy\n",
        "\n",
        "# and load the small english language model. Large models can be downloaded for many languages.\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "# find more models for other languages here: https://spacy.io/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69efVU1zOxcX"
      },
      "source": [
        "doc = nlp(X_test[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXj5vgBLQVcY"
      },
      "source": [
        "Spacy docs have POS (part of speech) and ENT (entity anotation) - let's see how we can use that to filter (bootstrap) a nice dictionary for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDUStuDfQFhM"
      },
      "source": [
        "# let's look at the POS tags\n",
        "[(tok.text, tok.pos_) for tok in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GQZBLCtPGze"
      },
      "source": [
        "# Let's tokenize the first 2000 articles (that should take around 1 minute with this approach)\n",
        "tokenlist = []\n",
        "for doc in nlp.pipe(X_train[:2000], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "  tokens =[tok.text.lower() for tok in doc if tok.pos_ in ['NOUN','ADJ','ADV','VERB'] and not tok.is_stop]\n",
        "  tokenlist.append(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5KlcFaZV2TB"
      },
      "source": [
        "from gensim.corpora.dictionary import Dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CA4GT2YV09a"
      },
      "source": [
        "dictionary = Dictionary(tokenlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb3gaqQKY2Tx"
      },
      "source": [
        "len(dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpJMiYr4Yo3o"
      },
      "source": [
        "dictionary.filter_extremes(no_below=5, no_above=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxGlszLcYwf7"
      },
      "source": [
        "len(dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHcG688SYKmZ"
      },
      "source": [
        "vectorizer = TfidfVectorizer(vocabulary=list(dictionary.values()))\n",
        "X_train_vec_2 = vectorizer.fit_transform(X_train)\n",
        "\n",
        "model = LogisticRegression(max_iter=2000)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "model.fit(X_train_vec_2, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx56cq-TbOMS"
      },
      "source": [
        "X_test_vec_2 = vectorizer.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUUb3QMtYk4x"
      },
      "source": [
        "# Check performance of the model\n",
        "model.score(X_test_vec_2, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOti4hZAbgWa"
      },
      "source": [
        "eli5.show_weights(model, feature_names=vectorizer.get_feature_names(), target_names=['negative','positive'], top=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T03_qkYbbiJb"
      },
      "source": [
        "eli5.show_prediction(model, X_test[0], vec=vectorizer, target_names=['negative','positive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Groo_9EFgKL-"
      },
      "source": [
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "tfidf = TfidfVectorizer(vocabulary=list(dictionary.values()))\n",
        "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
        "clf = MLPClassifier(verbose=False)\n",
        "\n",
        "\n",
        "pipe = make_pipeline(tfidf, svd, clf)\n",
        "\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdd4AzAvhHD2"
      },
      "source": [
        "pipe.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gOPVFZlbFV"
      },
      "source": [
        "from eli5.lime import TextExplainer\n",
        "\n",
        "te = TextExplainer(random_state=42)\n",
        "te.fit(X_test[0], pipe.predict_proba)\n",
        "te.show_prediction(target_names=['negative','positive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48efM-xWLmr_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDsYLS3apKAV"
      },
      "source": [
        "## Semantic search\n",
        "\n",
        "Once you obtain dense vectors that represent your text you can calculate distance measures. Where distance is not high, you will probably find texts that are semantically similar... :-)\n",
        "\n",
        "This can be done by calculating all distances in the corpus (which would be rather inefficient) or by using nearest-neighbor approximation.\n",
        "\n",
        "We will be using Annoy, a popular technique for finding neighbors developed at spotify (to find similar songs)\n",
        "https://github.com/spotify/annoy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DDacQ5eoWQ0"
      },
      "source": [
        "!pip install annoy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFClWp0BqQr6"
      },
      "source": [
        "Let's first vectorise our texts. For this we will be using Gensim, as it provides a more language-oriented approach as well as a good interlude into topic modelling..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdnKe01hqPdV"
      },
      "source": [
        "# Import the dictionary builder\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Import the TfidfModel from Gensim\n",
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "\n",
        "# Just like before, we import the model\n",
        "from gensim.models.lsimodel import LsiModel\n",
        "\n",
        "# Tooling to map between corpus (gensim) and matrix - more general\n",
        "from gensim.matutils import corpus2csc, corpus2dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp6g6oGwr4rk"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnqxsvWWrL-M"
      },
      "source": [
        "# Generate a dictionary and filter\n",
        "dictionary = Dictionary(tokenlist)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvos6LKMrghJ"
      },
      "source": [
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(word_tokenize(doc.lower())) for doc in data['review']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCG7Ty4otPCa"
      },
      "source": [
        "# Create and fit a new TfidfModel using the corpus: tfidf\n",
        "tfidf = TfidfModel(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCB4RgXutUTg"
      },
      "source": [
        "# transform corpus to TFIDF\n",
        "corpus_tfidf = tfidf[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur-cWfgitY_m"
      },
      "source": [
        "# Training the LSI model\n",
        "model_lsi = LsiModel(corpus_tfidf, num_topics = 300, id2word=dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o2P2-8vtti5"
      },
      "source": [
        "# Generating the corpus train & test\n",
        "\n",
        "corpus_lsi = model_lsi[corpus_tfidf]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8BVcOL2uAwg"
      },
      "source": [
        "# turn into matrix\n",
        "corpus_lsi_matrix = corpus2dense(corpus_lsi, 300 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CZg4ZVIuJFH"
      },
      "source": [
        "corpus_lsi_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Um8LMcuN4v"
      },
      "source": [
        "corpus_lsi_matrix = corpus_lsi_matrix.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwa-0qtaqATo"
      },
      "source": [
        "from annoy import AnnoyIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2IUDJHKp7VI"
      },
      "source": [
        "f = 300\n",
        "\n",
        "t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
        "\n",
        "for i in range(len(corpus_lsi_matrix)):\n",
        "    t.add_item(i, corpus_lsi_matrix[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE_zEbV4vQok"
      },
      "source": [
        "t.build(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmucxLUzvVWW"
      },
      "source": [
        "t.get_nns_by_item(0, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0cztZxnveaG"
      },
      "source": [
        "data['review'][44]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2sPdwm_v43W"
      },
      "source": [
        "data['review'][t.get_nns_by_item(44, 10)]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}