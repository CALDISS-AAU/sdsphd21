{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CuratingNetworks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pO93Fx2VYLz"
      },
      "source": [
        "# Network curation\n",
        "In this notebook we will use API calls to harvest data from Wikipedia. We will then use the NetworkX library to turn that data into different types of networks. The networks will be exported as .gexf files that can be visualized in the open source software [Gephi](https://gephi.org/users/download/) or, if you want a quick and dirty result without having to install any software, in the web application [Gephisto](https://jacomyma.github.io/gephisto/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlHaYXRtWwu1"
      },
      "source": [
        "### 1. Install and import the necessary libraries\n",
        "First, we will ensure that we have the right libraries installed and import them to this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZpEYjQ1VLpI",
        "outputId": "2b7df706-532c-43ad-805c-6c40988ffcab"
      },
      "source": [
        "!pip install wikipedia-api\n",
        "!pip install wikipedia\n",
        "!pip install pandas\n",
        "!pip install networkx\n",
        "\n",
        "import wikipediaapi\n",
        "import wikipedia\n",
        "import pandas as pd\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from wikipedia-api) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (1.24.3)\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=b753f3c60e2559bbb0fa311f86be41a9c5f6a2106cb67bb8b23154a3a1558159\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (2.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zez0IlVMX9WX"
      },
      "source": [
        "### 2. Get a set of Wikipedia articles\n",
        "Before we can start building networks we need a set of Wikipedia articles to work on. In the example below we get all the articles from the category on [\"computer ethics\"](https://en.wikipedia.org/wiki/Category:Computer_ethics). You could change the script to get articles from a different category. We call the an API endpoint called \"categorymembers\" to get this information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xz9V-GVPYvh1",
        "outputId": "447917a7-f9a1-497d-913d-48bbb2e28d30"
      },
      "source": [
        "# Decide which category to get articles from\n",
        "category_to_extract = \"Category:Computer_ethics\"\n",
        "\n",
        "# Create an empty set of articles to fill later on\n",
        "article_set = set()\n",
        "\n",
        "# This is an object we use to connect to the API.\n",
        "# Note that we configure it to use the English Wikipedia.\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    language='en',\n",
        "    extract_format = wikipediaapi.ExtractFormat.WIKI\n",
        ")\n",
        "\n",
        "# Create the category object (stuff specific to the API library)\n",
        "cat = wiki_wiki.page(category_to_extract)\n",
        "\n",
        "# Recursively build the list of pages (because there are sub-categories)\n",
        "# For the recursion, we create a function that might call itself\n",
        "def parse_categorymembers(categorymembers, level=0, max_level=2):\n",
        "    for c in categorymembers.values():\n",
        "        if c.ns == wikipediaapi.Namespace.MAIN: # This element is an article\n",
        "            article_set.add(c.title)\n",
        "        if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level: # This element is a sub-category\n",
        "            parse_categorymembers(c.categorymembers, level=level + 1, max_level=max_level)\n",
        "parse_categorymembers(cat.categorymembers)\n",
        "\n",
        "# Transform the set into a data frame for convenience\n",
        "article_df = pd.DataFrame(article_set, columns=[\"Article\"])\n",
        "\n",
        "# Output the data frame to check if it works\n",
        "article_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fake news website</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Doxing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISP redirect page</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pills, porn and poker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tencent Dajia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>The Memory Hole (website)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541</th>\n",
              "      <td>Whitelisting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542</th>\n",
              "      <td>Upstream collection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>Mimecast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>544</th>\n",
              "      <td>Godwin's law</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>545 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Article\n",
              "0            Fake news website\n",
              "1                       Doxing\n",
              "2            ISP redirect page\n",
              "3        Pills, porn and poker\n",
              "4                Tencent Dajia\n",
              "..                         ...\n",
              "540  The Memory Hole (website)\n",
              "541               Whitelisting\n",
              "542        Upstream collection\n",
              "543                   Mimecast\n",
              "544               Godwin's law\n",
              "\n",
              "[545 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BK45rSDZy35"
      },
      "source": [
        "## 3. Build two different networks\n",
        "We are now going to build to types of networks between the articles we have just extracted. They are both going to be monopartite networks where the nodes are the articles. The edges, however, will be built in two different ways.\n",
        "\n",
        "* In the first network, the edges will be the hyperlinks between the articles. This will show us how articles about computer ethics refer to each other. This network will be unweighted.\n",
        "\n",
        "* In the second network, the edges will represent the degree to which two articles refer to the same external references. This is essentially a projected bipartite network (we only see one type of nodes - the articles - but they are connected only through their connections to a second type of nodes - the external references - that we do not see here). The network, therefore, will be weighted (some articles have many references in common, while other articles have fewer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j720DegYbSPm"
      },
      "source": [
        "### 3.1.1 Get all the hyperlinks from the articles\n",
        "For the first network we will begin by calling the Wikipedia API to get all the hyperlinks from each article. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yuhUU74Y2PT",
        "outputId": "8ea83bee-40ab-4017-b279-95d4959074a4"
      },
      "source": [
        "cat_members_all=[]\n",
        "for each in article_df['Article']:\n",
        "  cat_members_all.append(each)\n",
        "\n",
        "lan=\"en\"\n",
        "\n",
        "seen = []\n",
        "network = {}\n",
        "print(\"Harvesting all links from \"+str(len(cat_members_all))+\" wikipedia pages. This might take a while...\")\n",
        "print(\"\")\n",
        "\n",
        "count=1\n",
        "for title in cat_members_all:\n",
        "    if count % 50 == 0:\n",
        "        print(\"All links harvested from \"+str(count)+\" pages out of \"+str(len(cat_members_all))+\". Continuing harvest...\")\n",
        "    if not title in seen:\n",
        "        seen.append(title)\n",
        "        try:\n",
        "        \n",
        "            page=wiki_wiki.page(title)\n",
        "            text_links = []\n",
        "            links = page.links\n",
        "            for link_title in sorted(links.keys()):\n",
        "                text_links.append(link_title)\n",
        "            network.update({title:text_links})\n",
        "\n",
        "        except:\n",
        "            print('SKIPPED: '+title)\n",
        "            print(\"\")\n",
        "    count=count+1\n",
        "    \n",
        "print(\"All pages harvested!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harvesting all links from 545 wikipedia pages. This might take a while...\n",
            "\n",
            "All links harvested from 50 pages out of 545. Continuing harvest...\n",
            "All links harvested from 100 pages out of 545. Continuing harvest...\n",
            "All links harvested from 150 pages out of 545. Continuing harvest...\n",
            "All links harvested from 200 pages out of 545. Continuing harvest...\n",
            "All links harvested from 250 pages out of 545. Continuing harvest...\n",
            "All links harvested from 300 pages out of 545. Continuing harvest...\n",
            "All links harvested from 350 pages out of 545. Continuing harvest...\n",
            "All links harvested from 400 pages out of 545. Continuing harvest...\n",
            "All links harvested from 450 pages out of 545. Continuing harvest...\n",
            "All links harvested from 500 pages out of 545. Continuing harvest...\n",
            "All pages harvested...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXbfKxgHbiq-"
      },
      "source": [
        "### 3.1.2 Build the network of articles (nodes) connected by hyperlinks (edges)\n",
        "We then use the NetworkX library to buld the network and export the result as a .gexf file. Given that many of the hyperlinks point to articles outside the \"computer ethics\" category, we will first theck if a link is between two articles on our list before we include it as an edge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5br5DAAbfBr",
        "outputId": "18ecccd8-fd20-41e3-cbfe-633d60fd0b4c"
      },
      "source": [
        "membersonly_edges = []\n",
        "members = network.keys()\n",
        "print(\"Building network...\")\n",
        "print(\"\")\n",
        "for source in network:\n",
        "    for target in network[source]:\n",
        "        edge = (source,target)\n",
        "        if target in members:\n",
        "            membersonly_edges.append(edge)\n",
        "print(\"Saving network...\")\n",
        "print(\"\")\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from(membersonly_edges)\n",
        "nx.write_gexf(G,category_to_extract+'_AllLinksNet_membersonly.gexf')\n",
        "\n",
        "print('DONE!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating networks...\n",
            "\n",
            "Saving network...\n",
            "\n",
            "DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tBbIqIhc8j8"
      },
      "source": [
        "### 3.2.1 Get all the external references from the articles\n",
        "For the second network we will begin by calling the Wikipedia API to get all the external references from each article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXvDXwYJc7ca",
        "outputId": "f1663857-f6f6-4c01-fb05-3d7f66d3442b"
      },
      "source": [
        "cat_members_dict={}\n",
        "cat_members_list=[]\n",
        "for title in cat_members_all:\n",
        "    try:\n",
        "        page = wikipedia.page(title,auto_suggest=False)\n",
        "    except wikipedia.exceptions.DisambiguationError:\n",
        "        print(\"Wikipedia thinks \"+title+\" is ambiguous (returns several candidate pages). Trying again with all capitalized letters\")\n",
        "        try:\n",
        "            page = wikipedia.page(title.capitalize(),auto_suggest=False)\n",
        "            print(\"Success! \"+title+\" is no longer ambiguous\")\n",
        "        except wikipedia.exceptions.DisambiguationError:\n",
        "            print(\"Wikipedia still thinks \"+title+\" is ambiguous (returns several candidate pages). Trying again with all lower letters\")\n",
        "            try:\n",
        "                page = wikipedia.page(title.lower(),auto_suggest=False)\n",
        "                print(\"Success! \"+title+\" is no longer ambiguous\")\n",
        "            except wikipedia.exceptions.DisambiguationError:\n",
        "                print(\"Wikipedia still thinks \"+title+\" is ambiguous (returns several candidate pages). Skipping page...\")\n",
        "                continue\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(\"The page \"+title+\" could not be found. Skipping page...\")\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        \n",
        "    try:\n",
        "        refs = page.references\n",
        "      #  print(target_refs)\n",
        "        cat_members_dict[title]={\"references\":refs}\n",
        "        cat_members_list.append(title)\n",
        "\n",
        "    except KeyError:\n",
        "        print(\"Could not retrieve references for \"+title+\". Skipping page...\")\n",
        "        continue\n",
        "print(\"Succesfully retrieved references from \"+str(len(cat_members_dict))+\" out of \"+str(len(cat_members_all))+\" wikipedia pages. Generating network....\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wikipedia thinks Comment spam is ambiguous (returns several candidate pages). Trying again with all capitalized letters\n",
            "Wikipedia still thinks Comment spam is ambiguous (returns several candidate pages). Trying again with all lower letters\n",
            "Wikipedia still thinks Comment spam is ambiguous (returns several candidate pages). Skipping page...\n",
            "Succesfully retrieved references from 544 out of 545 wikipedia pages. Generating network....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjmZN0KsfgEM"
      },
      "source": [
        "### 3.2.2 Build the network of articles (nodes) connected by shared references (edges)\n",
        "We then use the NetworkX library to buld the network and export the result as a .gexf file. Given that articles have varying numbers of external references in common, we will weight the edges to reflect the volume of shared references between two nodes.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxzSS_Mdd2Jq",
        "outputId": "9cf19f95-e8e1-4d4f-a440-52c0eb20a7bc"
      },
      "source": [
        "edges = []\n",
        "\n",
        "for i,source in enumerate(cat_members_list):\n",
        "    source_refs = cat_members_dict[source][\"references\"]\n",
        "    if len(source_refs)>0:\n",
        "        for target in cat_members_list[i+1:]:\n",
        "            if target==source:\n",
        "                continue\n",
        "            target_refs=cat_members_dict[target][\"references\"]\n",
        "            if len(target_refs)>0:\n",
        "                overlap = len(set(source_refs).intersection(target_refs))\n",
        "                if overlap>0:\n",
        "                    if len(source_refs) < len(target_refs):\n",
        "                        norm_overlap_by_smallest = overlap / len(source_refs)\n",
        "                    else:\n",
        "                        norm_overlap_by_smallest = overlap / len(target_refs)\n",
        "                    edge = (source,target,{'overlap':overlap,'norm_overlap_by_smallest':norm_overlap_by_smallest})\n",
        "                    edges.append(edge)\n",
        "print(\"Network has been generated. Saving...\")\n",
        "G = nx.Graph()\n",
        "G.add_edges_from(edges)\n",
        "nx.write_gexf(G,category_to_extract+'_CoReferenceNet_membersonly.gexf')\n",
        "print(\"DONE!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network has been generated. Saving...\n",
            "DONE!\n"
          ]
        }
      ]
    }
  ]
}