{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised ML and NLP-caldissNLP",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CALDISS-AAU/sdsphd21/blob/master/notebooks/Unsupervised_ML_and_NLP_caldissNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDq-VGAB_WG_"
      },
      "source": [
        "# Moving on to text-vectorization\n",
        "\n",
        "Before we dive into text-vectorization and representation: A bit something on SpaCy and part-of-speech tagging. Also: We will be using SpaCy as our main pre-processing tool, as it combines most of the things that we need in one convinient api.\n",
        "\n",
        "![](https://assets.vogue.com/photos/613fd623fdb2d0f49a50f0a5/master/w_2560%2Cc_limit/GettyImages-1340129801.jpeg)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0-7_Zl8aaMM"
      },
      "source": [
        "!pip install pandas --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZMq7F2oBopo"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTqqjxP9_uAR"
      },
      "source": [
        "# Here is a long article text - again\n",
        "article = \"\"\"Lil Nas X—Montero, Monte, or Nas to the friends he still has—knew before I even arrived at the San Vicente Bungalows in West Hollywood that he was where he was meant to be. No one had to tell him who I was. The universe had informed him, as he arrived and saw his lucky numbers—7 and 9—on the license plate of the car in front of him, that his future depended on him being here, right now. I, unfortunately, am not fluent in the universe. I’ve never felt I heard it speak to me, even as so many of my intuitions led me down paths toward great fortune. So as I arrived to the elite social club, which at one point was a haven for WeHo’s seediest gay hookups, I had no idea where I was meant to be. I only knew that after getting on an airplane, hungover in a post–Tony Awards weekend haze, I didn’t particularly want to be anywhere, let alone with our country’s biggest pop star, a young man a decade younger than me, interviewing him about his many successes.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0D7XFT4BOlK"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "# Let's apply the model to the article (as easy as that)\n",
        "article_nlp = nlp(article)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZctG-sGDltL"
      },
      "source": [
        "# spaCy also splits sentences\n",
        "[sentance for sentance in article_nlp.sents][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3m4lIm44qv6"
      },
      "source": [
        "# And: it will also annotate them with POS-labels\n",
        "sentance = [sentance for sentance in article_nlp.sents][4]\n",
        "[(token.text, token.pos_) for token in sentance]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou1jGUv74843"
      },
      "source": [
        "# We can use this to extract only tokens that we think bear most of the meaning\n",
        "[token.text for token in sentance if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] and not token.is_stop] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFiFOnvEW5Oe"
      },
      "source": [
        "# Also, we can create lemmas, thus reducing heterogeneity in the vocabulary without sacrificing much meaning\n",
        "\n",
        "[tok.lemma_ for tok in nlp(\"a text about innovations and all kinds things which goes nowhere\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvo7rxqOTIF9"
      },
      "source": [
        "# Isn't that great?\n",
        "\n",
        "[token.lemma_.lower() for token in sentance if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] and not token.is_stop] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ33e5O8ic-c"
      },
      "source": [
        "Thus we have created a representation of a text that is probably as \"minimal\" as possible - Maximising meaning and minimising \"noise\"\n",
        "\n",
        "In the last part of this notebook we will try to use such representations to explore the content of text collections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf1JFlnmaSUB"
      },
      "source": [
        "### Bag of words model\n",
        "\n",
        "In order for a computer to understand text we need to somehow find a useful representation.\n",
        "If you need to compare different texts e.g. articles, you will probably go for keywords. These keywords may come from a keyword-list with for example 200 different keywords\n",
        "In that case you could represent each document with a (sparse) vector with 1 for \"keyword present\" and 0 for \"keyword absent\"\n",
        "We can also get a bit more sophoistocated and count the number of times a word from our dictionary occurs.\n",
        "For a corpus of documents that would give us a document-term matrix\n",
        "![example](https://i.stack.imgur.com/C1UMs.png)\n",
        "\n",
        "Let's try creating a bag of words model from our initial example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64CPbz3xaWpp"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    \n",
        "     'A text about cats.',\n",
        "     'A text about dogs.',\n",
        "     'And another text about a dog.',\n",
        "     'Why always writing about cats and dogs, always dogs?',\n",
        "   ]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pCTn2y6a2ra"
      },
      "source": [
        "pd.DataFrame(X.A, columns=vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roRMrE7HpNHp"
      },
      "source": [
        "#### TF-IDF - Term Frequency - Inverse Document Frequency\n",
        "\n",
        "A token is importan for a document if appears very often\n",
        "A token becomes less important for comparaison across a corpus if it appears all over the place in the corpus\n",
        "\n",
        "*Innovation* in a corpus of abstracts talking about innovation is not that important\n",
        "\n",
        "\\begin{equation*}\n",
        "w_{i,j} = tf_{i,j}*log(\\frac{N}{df_i})\n",
        "\\end{equation*}\n",
        "\n",
        "- $w_{i,j}$ = the TF-IDF score for a term i in a document j\n",
        "- $tf_{i,j}$ = number of occurence of term i in document j\n",
        "- $N$ = number of documents in the corpus\n",
        "- $df_i$ = number of documents with term i\n",
        "\n",
        "\n",
        "We will use TF-IDF to transform our corpus. However, first we need to fir the TF-IDF model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2TjxKuHnEzJ"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDln35hvo7qx"
      },
      "source": [
        "pd.DataFrame(X.A, columns=vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHV7quvyL0cd"
      },
      "source": [
        "# let's fist install this nice visualizer\n",
        "!pip install -qq pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuGZe88uMxCh"
      },
      "source": [
        "# and import it\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "%matplotlib inline\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmAhAMcrjJjU"
      },
      "source": [
        "We will be using a dataset from EU Cordis which describes H2020 research projects. No tweets for now.\n",
        "\n",
        "http://data.europa.eu/euodp/en/data/dataset/cordisH2020projects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZyun0T2US-a"
      },
      "source": [
        "# I put a little sample (500 observations) of the data on github\n",
        "\n",
        "reports = pd.read_csv('https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/cordis-h2020reports.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5aD1YE3dHNe"
      },
      "source": [
        "reports.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLXIRn3Pl3_h"
      },
      "source": [
        "# reindec\n",
        "reports.index = range(len(reports))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9ZYojgcX8xx"
      },
      "source": [
        "# now, let's combine everything that we learned about preprocessing in a few lines of code\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(reports['summary'], disable=[\"parser\", \"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swYK2bBvbbol"
      },
      "source": [
        "# Let's bring the tokens back in\n",
        "\n",
        "reports['tokens'] = tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b0TwvW8DTxB"
      },
      "source": [
        "reports['tokens'][3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPG-IPHRkkV9"
      },
      "source": [
        "Another library that you have to know when doing NLP (once you progress to DeepLearning and recent stuff probably not any more but for now) is gensim\n",
        "\n",
        "https://radimrehurek.com/gensim/\n",
        "\n",
        "This is the library that handles all kinds of statistical NLP tasks and goes as far as implementing (super efficient) embedding model training (next class)\n",
        "But: With NLP today being all BERT, ELMO and transformers probably declining in importance. Back in 2013 gensim was a major discovery and breakthrough helper when I was working on my PhD. One more reason to have a look at it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "motjLUBpDmdL"
      },
      "source": [
        "!pip install -qq -U gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWU7zITlkwPs"
      },
      "source": [
        "# Import the dictionary builder\n",
        "from gensim.corpora.dictionary import Dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOmNmWMvZ8Vy"
      },
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(reports['tokens'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZW6Idn_btnL"
      },
      "source": [
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 1000 words\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB6aB8E-bkW9"
      },
      "source": [
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in reports['tokens']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNOI_-xNl5PG"
      },
      "source": [
        "# That's how the corpus looks\n",
        "corpus[3][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4krg3HfmAhp"
      },
      "source": [
        "### Topic modelling - NLP meets unsupervised ML\n",
        "\n",
        "The corpus is a list of tuples, with word-ids and the number of their occurrence in documents: LDA - https://youtu.be/DWJYZq_fQ2A\n",
        "\n",
        "We will start with a topic modelling approach that is good for interpretable topics but not too much for further processing\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1600/1*pZo_IcxW1GVuH2vQKdoIMQ.jpeg)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3vcFRRqbsFR"
      },
      "source": [
        "# we'll use the faster multicore version of LDA\n",
        "\n",
        "from gensim.models import LdaMulticore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv_Z6JXIb526"
      },
      "source": [
        "# Training the model\n",
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=10, workers = 4, passes=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXPrQHO9nDhB"
      },
      "source": [
        "# Check out topics\n",
        "lda_model.print_topics(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d80U1-IrnK77"
      },
      "source": [
        "# Where does a text belong to?\n",
        "lda_model[corpus][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67BsAOEMtAO9"
      },
      "source": [
        "reports['summary'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDIH34Lvb7Cz"
      },
      "source": [
        "# Let's try to visualize\n",
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmDFJBBFcBHV"
      },
      "source": [
        " # Let's Visualize\n",
        "pyLDAvis.display(lda_display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA3LS0RQETxM"
      },
      "source": [
        "# In case you run a website and want to publish it...or embed it in a blogpost...\n",
        "pyLDAvis.save_html(lda_display, 'lda.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYOPWeMLpMcz"
      },
      "source": [
        "# And that's how you get the topic-number that's ranked highest\n",
        "\n",
        "print(sorted([(2, 0.121567), (9, 0.8610384)], key=lambda x: -x[1]))\n",
        "print(sorted([(2, 0.121567), (9, 0.8610384)], key=lambda x: -x[1])[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A6aJYDXoorZ"
      },
      "source": [
        "From here, you can assign topics to texts...do some EDA, explore how topics evolve over time etc.\n",
        "\n",
        "Finally, let's try out LSA (an older topic-moddeling approach similar to NMF) - thus unsupervised ML\n",
        "\n",
        "More on LDA: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76xo7pFWI4AB"
      },
      "source": [
        "### Your Turn:\n",
        "\n",
        "![alt text](https://media.giphy.com/media/1zjRp3fs05jhjTuwr3/giphy.gif)\n",
        "\n",
        "Perform an LDA analysis of the #Grammy's dataset\n",
        "\n",
        "- Filter the corpus using `tweet-preprocessor` - try to figure out how to use it using it's documentation\n",
        "- Clean up further with SpaCy (keep only ADV, ADJ, NOUN)\n",
        "- Use Gensim to build a Dictionary (Filter extremes) and Corpus\n",
        "- Use Gensim to run LDA\n",
        "- Identify 10 topics\n",
        "- Plot topic-counts by 2 minute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZPir4Cmb2_K"
      },
      "source": [
        "### LSI models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIc4Nd0SqZ7a"
      },
      "source": [
        "# Import the TfidfModel from Gensim\n",
        "from gensim.models.tfidfmodel import TfidfModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ABvkfSjd4ly"
      },
      "source": [
        "# Create and fit a new TfidfModel using the corpus: tfidf\n",
        "tfidf = TfidfModel(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_geBUz3eJ8O"
      },
      "source": [
        "# Now we can transform the whole corpus\n",
        "tfidf_corpus = tfidf[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k36xISjteOkw"
      },
      "source": [
        "# Just like before, we import the model\n",
        "from gensim.models.lsimodel import LsiModel\n",
        "\n",
        "# And we fir it on the tfidf_corpus pointing to the dictionary as reference and the number of topics.\n",
        "# In more serious settings one would pick between 300-400\n",
        "lsi = LsiModel(tfidf_corpus, id2word=dictionary, num_topics=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDMZkYbCeTda"
      },
      "source": [
        "lsi.show_topics(num_topics=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj_nxOcveWuN"
      },
      "source": [
        "# And just as before, we can use the trained model to transform the corpus\n",
        "lsi_corpus = lsi[tfidf_corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpyEfcy9eeXk"
      },
      "source": [
        "# Load the MatrixSimilarity\n",
        "from gensim.similarities import MatrixSimilarity\n",
        "\n",
        "# Create the document-topic-matrix\n",
        "document_topic_matrix = MatrixSimilarity(lsi_corpus)\n",
        "document_topic_matrix_ix = document_topic_matrix.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI8K24W2lTcP"
      },
      "source": [
        "# this now allows us to perform similarity-queries\n",
        "\n",
        "sims = document_topic_matrix[lsi_corpus[0]]\n",
        "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "print(sims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvNB5tGWrtEi"
      },
      "source": [
        "We will go deeper into how that works next time\n",
        "The last bit is a bit of a quick bonus and should be super familiar from M1.\n",
        "\n",
        "Since we now have a matrix with observations and features - why not trying to apply unsupervisd ML that we know from M1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWE0SBQ-cDcs"
      },
      "source": [
        "!pip install -q umap-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBn8C2lIefgC"
      },
      "source": [
        "# dimensionalility reduction for plotting\n",
        "import umap\n",
        "\n",
        "embeddings = umap.UMAP(n_neighbors=15, metric='cosine').fit_transform(document_topic_matrix_ix)\n",
        "\n",
        "#------------------------------\n",
        "# we could use that too\n",
        "\n",
        "#from sklearn.decomposition import PCA\n",
        "\n",
        "#reduced = PCA(n_components = 10).fit_transform(document_topic_matrix_ix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlDGWflf5ng"
      },
      "source": [
        "# Nothing new here\n",
        "from sklearn.cluster import KMeans\n",
        "clusterer = KMeans(n_clusters = 10)\n",
        "clusterer.fit(document_topic_matrix_ix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCRVRPcygpnG"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9y77tBVe3VL"
      },
      "source": [
        "# Plotting things\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "plt.figure(figsize=(12,12))\n",
        "g = sns.scatterplot(*embeddings.T,\n",
        "                    #reduced[:,0],reduced[:,1],\n",
        "                   hue=clusterer.labels_,\n",
        "                    palette=\"Paired\",\n",
        "                   legend='full')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQYukJYIgYwY"
      },
      "source": [
        "# Let's explore the clusters ... that should actually correlate with topics found by LDA\n",
        "reports['cluster'] = clusterer.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3chqB3Sg-8_"
      },
      "source": [
        "reports[reports['cluster'] == 2]['teaser']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hnaBLL0gtCf"
      },
      "source": [
        "from gensim.matutils import corpus2dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o_i5_NGj8gS"
      },
      "source": [
        "# Let's check out the topics by getting \"top-tfidf\" for the different clusters (and we need to transponse)\n",
        "tfidf_matrix = corpus2dense(tfidf_corpus, len(dictionary)).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkyGeh87khvL"
      },
      "source": [
        "# write cluster-numbers into our data\n",
        "reports['cluster'] = clusterer.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgCiVflWkpAF"
      },
      "source": [
        "# Get indices to subset the matrix\n",
        "cluster_index = reports[reports['cluster'] == 2].index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mffPu-f_zF0o"
      },
      "source": [
        "tfidf_matrix[cluster_index,:].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UquKhoAvk8Ju"
      },
      "source": [
        "# Use numpy to sum up columns for tfidf, get the indices of the sorted values, and flip (descending), and only top 10\n",
        "topk = np.flip(np.argsort(np.sum(tfidf_matrix[cluster_index,:], axis=0)))[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNOoxP9jlIEn"
      },
      "source": [
        "# Use dictionary to get the words from indices\n",
        "[dictionary[x] for x in topk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBnWPpnSlT2Z"
      },
      "source": [
        "# Let's loop it\n",
        "\n",
        "for i in set(clusterer.labels_):\n",
        "  cluster_index = reports[reports['cluster'] == i].index\n",
        "  topk = np.flip(np.argsort(np.sum(tfidf_matrix[cluster_index,:], axis=0)))[:10]\n",
        "\n",
        "  print(str(i) + str([dictionary[x] for x in topk]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}